{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "1d93886d-9542-492e-bb6e-e2b0662f98ac",
      "metadata": {
        "id": "1d93886d-9542-492e-bb6e-e2b0662f98ac"
      },
      "outputs": [],
      "source": [
        "!pip install exllamav2 --quiet\n",
        "# you may need to install git-lfs first if you run into issues (see https://git-lfs.com/)\n",
        "!git clone --branch 4.0bpw https://huggingface.co/turboderp/Mistral-7B-instruct-exl2 # see https://huggingface.co/turboderp for other models"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "ca92ed35-eb32-456e-b8e7-07eee269a0f4",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ca92ed35-eb32-456e-b8e7-07eee269a0f4",
        "outputId": "30820442-c1fb-47f0-afea-a9c916d75c70"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading model: Mistral-7B-instruct-exl2\n"
          ]
        }
      ],
      "source": [
        "# From https://github.com/turboderp/exllamav2/blob/master/examples/inference.py (with minor edits)\n",
        "\n",
        "import sys, os\n",
        "\n",
        "from exllamav2 import(\n",
        "    ExLlamaV2,\n",
        "    ExLlamaV2Config,\n",
        "    ExLlamaV2Cache,\n",
        "    ExLlamaV2Tokenizer,\n",
        ")\n",
        "\n",
        "from exllamav2.generator import (\n",
        "    ExLlamaV2BaseGenerator,\n",
        "    ExLlamaV2Sampler\n",
        ")\n",
        "\n",
        "import time\n",
        "\n",
        "# Initialize model and cache\n",
        "\n",
        "model_directory =  \"Mistral-7B-instruct-exl2\"\n",
        "\n",
        "config = ExLlamaV2Config()\n",
        "config.model_dir = model_directory\n",
        "config.prepare()\n",
        "\n",
        "model = ExLlamaV2(config)\n",
        "print(\"Loading model: \" + model_directory)\n",
        "\n",
        "cache = ExLlamaV2Cache(model, lazy = True)\n",
        "model.load_autosplit(cache)\n",
        "\n",
        "tokenizer = ExLlamaV2Tokenizer(config)\n",
        "\n",
        "# Initialize generator\n",
        "\n",
        "generator = ExLlamaV2BaseGenerator(model, cache, tokenizer)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "d6e28b94-f13d-4884-a443-5c90a8c768d9",
      "metadata": {
        "id": "d6e28b94-f13d-4884-a443-5c90a8c768d9"
      },
      "outputs": [],
      "source": [
        "# Configure generator\n",
        "\n",
        "settings = ExLlamaV2Sampler.Settings()\n",
        "settings.temperature = 0.85\n",
        "settings.top_k = 50\n",
        "settings.top_p = 0.8\n",
        "settings.token_repetition_penalty = 1.05\n",
        "settings.disallow_tokens(tokenizer, [tokenizer.eos_token_id])\n",
        "\n",
        "generator.warmup()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Generate some text\n",
        "\n",
        "prompt = \"In this MBA workshop about ML and AI, we will\"\n",
        "\n",
        "max_new_tokens = 250\n",
        "\n",
        "time_begin = time.time()\n",
        "\n",
        "output = generator.generate_simple(prompt, settings, max_new_tokens, seed = 1234)\n",
        "\n",
        "time_end = time.time()\n",
        "time_total = time_end - time_begin\n",
        "\n",
        "print(output)\n",
        "print()\n",
        "print(f\"Response generated in {time_total:.2f} seconds, {max_new_tokens} tokens, {max_new_tokens / time_total:.2f} tokens/second\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DFBPg0Iw302b",
        "outputId": "7a497ae5-0f30-49a0-a2d7-9a204ae69448"
      },
      "id": "DFBPg0Iw302b",
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "In this MBA workshop about ML and AI, we will also be discussing some real-world applications of these technologies in business. We will be exploring how businesses can use ML and AI to improve their operations, enhance customer experience, and gain a competitive advantage.\n",
            "\n",
            "We will also be discussing ethical considerations related to ML and AI, such as privacy concerns, bias, and transparency. It is important for businesses to understand these issues and develop strategies to address them in order to build trust with customers and maintain ethical practices.\n",
            "\n",
            "Overall, this MBA workshop about ML and AI will provide participants with a comprehensive understanding of these technologies and their potential applications in business. Participants will leave with practical insights and strategies for implementing ML and AI in their organizations, as well as an awareness of the ethical considerations that come with these technologies.\n",
            "\n",
            "### Learning Objectives:\n",
            "\n",
            "- Understand the basics of machine learning and artificial intelligence\n",
            "- Identify potential applications of ML and AI in business\n",
            "- Develop practical strategies for implementing ML and AI in business\n",
            "- Explore ethical considerations related to ML and AI\n",
            "- Build a network of colleagues and industry leaders interested in ML and AI\n",
            "\n",
            "### Target Audience:\n",
            "\n",
            "This MBA workshop is designed for business professionals who want\n",
            "\n",
            "Response generated in 2.71 seconds, 250 tokens, 92.42 tokens/second\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "78_zmeWJ5EYs"
      },
      "id": "78_zmeWJ5EYs",
      "execution_count": 4,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.12"
    },
    "colab": {
      "provenance": [],
      "gpuType": "A100"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 5
}