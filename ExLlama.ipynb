{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1d93886d-9542-492e-bb6e-e2b0662f98ac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fatal: destination path 'Vigogne-2-13B-Instruct-GPTQ' already exists and is not an empty directory.\n"
     ]
    }
   ],
   "source": [
    "!pip install sentencepiece safetensors --quiet\n",
    "!pip install https://github.com/jllllll/exllama/releases/download/0.0.9/exllama-0.0.9+cu118-cp310-cp310-linux_x86_64.whl --quiet\n",
    "\n",
    "# you may need to install git-lfs first if you run into issues (see https://git-lfs.com/)\n",
    "!git clone https://huggingface.co/TheBloke/Vigogne-2-13B-Instruct-GPTQ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ca92ed35-eb32-456e-b8e7-07eee269a0f4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "In this MBA workshop about ML and AI, we will be covering the following topics:\n",
      "1. What is Machine Learning? (Definition)\n",
      "2. Types of Machine Learning Models\n",
      "3. Supervised vs Unsupervised learning\n",
      "4. Classification & Regression models in Machine Learning\n",
      "5. Artificial Intelligence - what it means for businesses today\n",
      "6. The Future of Work â€“ How Automation affects Jobs\n",
      "7. Chatbots & Conversational Interfaces\n",
      "8. Deep Learning\n",
      "9. Predictive Analytics\n",
      "This session is ideal for individuals who are interested to learn more about machine learning, artificial intelligence or predictive analytics as well as professionals working with data who want to stay up-to-date on emerging trends. This includes executives, managers, entrepreneurs, IT staff, engineers, research scientists, consultants, analysts, marketers, salespeople, etc. No prior knowledge of statistics or programming languages required. All levels\n"
     ]
    }
   ],
   "source": [
    "# From https://github.com/jllllll/exllama/blob/master/example_basic.py (with minor edits)\n",
    "\n",
    "from exllama.model import ExLlama, ExLlamaCache, ExLlamaConfig\n",
    "from exllama.tokenizer import ExLlamaTokenizer\n",
    "from exllama.generator import ExLlamaGenerator\n",
    "import os, glob\n",
    "\n",
    "# Directory containing model, tokenizer, generator\n",
    "\n",
    "model_directory =  \"./Vigogne-2-13B-Instruct-GPTQ\"\n",
    "\n",
    "# Locate files we need within that directory\n",
    "\n",
    "tokenizer_path = os.path.join(model_directory, \"tokenizer.model\")\n",
    "model_config_path = os.path.join(model_directory, \"config.json\")\n",
    "st_pattern = os.path.join(model_directory, \"*.safetensors\")\n",
    "model_path = glob.glob(st_pattern)[0]\n",
    "\n",
    "# Create config, model, tokenizer and generator\n",
    "\n",
    "config = ExLlamaConfig(model_config_path)               # create config from config.json\n",
    "config.model_path = model_path                          # supply path to model weights file\n",
    "#config.set_auto_map(\"12,24\")                            # adjust as needed for dual gpu (etc.)\n",
    "\n",
    "model = ExLlama(config)                                 # create ExLlama instance and load the weights\n",
    "tokenizer = ExLlamaTokenizer(tokenizer_path)            # create tokenizer from tokenizer model file\n",
    "\n",
    "cache = ExLlamaCache(model)                             # create cache for inference\n",
    "generator = ExLlamaGenerator(model, tokenizer, cache)   # create generator\n",
    "\n",
    "# Configure generator\n",
    "\n",
    "generator.disallow_tokens([tokenizer.eos_token_id])\n",
    "\n",
    "generator.settings.token_repetition_penalty_max = 1.2\n",
    "generator.settings.temperature = 0.95\n",
    "generator.settings.top_p = 0.65\n",
    "generator.settings.top_k = 100\n",
    "generator.settings.typical = 0.5\n",
    "\n",
    "# Produce a simple generation\n",
    "\n",
    "prompt = \"In this MBA workshop about ML and AI, we will\"\n",
    "print (prompt, end = \"\")\n",
    "\n",
    "output = generator.generate_simple(prompt, max_new_tokens = 200)\n",
    "\n",
    "print(output[len(prompt):])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6e28b94-f13d-4884-a443-5c90a8c768d9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
